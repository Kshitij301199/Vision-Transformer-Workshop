{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b33bd7c-bca7-4c00-932a-2921e73bffe3",
   "metadata": {},
   "source": [
    "<img src=\"images/00-image.png\" alt=\"encoder\" class=\"bg-primary\" width=\"100%\">\n",
    "\n",
    "\n",
    "[Image Reference](https://www.planetware.com/tourist-attractions-/potsdam-d-br-pt.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18cc9e8-6f36-4a06-b602-180e5bb95cbd",
   "metadata": {},
   "source": [
    "<h1><center> Attention Explained <center></h1>\n",
    "\n",
    "Vision Transformer (ViT) paper: [Paper Reference](https://arxiv.org/abs/2010.11929)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65045704-07d8-4ac0-8926-d28c303ffe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbaadedd-950a-46f3-af78-e6ea0e416c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da93f65-d75e-406d-92aa-00f3325c8bfc",
   "metadata": {},
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d063c7f2-7bde-4e1c-a8e7-d3740be445fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we need to find a way to parameterize each token so that we can rank them based on their importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3064a66a-2b97-41d7-b0c3-be3a91b006ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "B, T, C = 4, 8, 2 # Batch, Time Dim, Channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "704a078e-071f-4a89-9403-96bc396c09b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873],\n",
       "        [ 0.9007, -2.1055],\n",
       "        [ 0.6784, -1.2345],\n",
       "        [-0.0431, -1.6047],\n",
       "        [-0.7521,  1.6487],\n",
       "        [-0.3925, -1.4036],\n",
       "        [-0.7279, -0.5594],\n",
       "        [-0.7688,  0.7624]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f0852-7eb1-4f7b-84d9-832b17cb67e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Version 1 : Using simple mathematical function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5353dddf-4e31-4f6e-9a70-14d8884f485a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention1 = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_previous = x[b, :t+1]\n",
    "        attention1[b, t] = torch.mean(x_previous, 0) # also called bow: bag of words / pixel or bop in out case\n",
    "\n",
    "# bow is an averaging schemes, thus used as an attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33be8d65-1883-4aff-972f-b90ae729924d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873],\n",
       "        [ 0.9007, -2.1055],\n",
       "        [ 0.6784, -1.2345],\n",
       "        [-0.0431, -1.6047]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, :3+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d13a4f61-abfc-4382-b486-54ff33853063",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873],\n",
       "        [ 1.4138, -0.3091],\n",
       "        [ 1.1687, -0.6176]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1[0, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce572b-d528-4868-99d7-553e7a46f8b0",
   "metadata": {},
   "source": [
    "### Replicating above by combining Matmul and Trill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7b93810-eff2-438b-a2ad-167e1e5f76d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Explaining concept of matmul - example\n",
    "a = torch.ones(3,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93d7caf1-3cbe-4358-9895-b7a9fc577dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [3., 0.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e29dfc48-2189-4524-9910-ad88e8fa8530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 2.],\n",
      "        [4., 2.],\n",
      "        [4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "c = a@b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed1d69c5-6a4e-4c28-aca8-748b8112c0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explaining concept of Tril - example\n",
    "torch.tril(torch.ones(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b6ed27f-e2c3-4b89-bbd8-d83eb3e5abc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.],\n",
      "        [0., 4.],\n",
      "        [0., 3.],\n",
      "        [8., 4.],\n",
      "        [0., 4.],\n",
      "        [1., 2.]])\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.],\n",
      "        [14., 20.],\n",
      "        [14., 23.],\n",
      "        [22., 27.],\n",
      "        [22., 31.],\n",
      "        [23., 33.]])\n"
     ]
    }
   ],
   "source": [
    "# Combining tril and matmul\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(8, 8))\n",
    "b = torch.randint(0, 10, (8, 2)).float()\n",
    "c = a@b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b157b2f4-eff0-4c40-8882-a93194ff44f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.],\n",
      "        [0., 4.],\n",
      "        [0., 3.],\n",
      "        [8., 4.],\n",
      "        [0., 4.],\n",
      "        [1., 2.]])\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333],\n",
      "        [3.5000, 5.0000],\n",
      "        [2.8000, 4.6000],\n",
      "        [3.6667, 4.5000],\n",
      "        [3.1429, 4.4286],\n",
      "        [2.8750, 4.1250]])\n"
     ]
    }
   ],
   "source": [
    "# average and using tril : basically make everything we multiply with it average themselves\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(8,8))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (8, 2)).float()\n",
    "c = a@b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c234214-7288-45c4-92d1-f8c6e111f46d",
   "metadata": {},
   "source": [
    "### Introducing and Establishing the SCORES / WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5131236-9f55-48b5-98ec-8be9aea2ed6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now appling it\n",
    "tril = torch.tril(torch.ones(T,T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92a01e08-b003-46ca-9cb3-6e697efe27a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5384d71c-e6ef-4261-b7e9-65a67d5da55c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = tril / torch.sum(tril, axis = 1, keepdim=True)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa3489af-753e-4a7f-a37c-267ee5ef924f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention2 = scores @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5b4aa47-09d5-4d80-9f19-22c651ef1078",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(attention2[0], attention1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18d9d6b6-14d2-4558-917d-1ce26bab2a32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873],\n",
       "        [ 1.4138, -0.3091],\n",
       "        [ 1.1687, -0.6176],\n",
       "        [ 0.8657, -0.8644],\n",
       "        [ 0.5422, -0.3617],\n",
       "        [ 0.3864, -0.5354],\n",
       "        [ 0.2272, -0.5388],\n",
       "        [ 0.1027, -0.3762]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30f79e82-2bb9-46c1-8fd2-fe257c34058b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873],\n",
       "        [ 1.4138, -0.3091],\n",
       "        [ 1.1687, -0.6176],\n",
       "        [ 0.8657, -0.8644],\n",
       "        [ 0.5422, -0.3617],\n",
       "        [ 0.3864, -0.5354],\n",
       "        [ 0.2272, -0.5388],\n",
       "        [ 0.1027, -0.3762]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bb78f3-b367-48d5-965f-f78db535e7af",
   "metadata": {},
   "source": [
    "- Now we have found a way to parameterize them such that we have our weight (average) separated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da4eb3-1f1a-408c-86b2-8b14ac8a16d1",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec147993-65e9-492f-ac1d-d716982fca4d",
   "metadata": {},
   "source": [
    "## Version 2 : Using softmax - another method but better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8651e-b06c-4806-a614-9510980b8be2",
   "metadata": {
    "tags": []
   },
   "source": [
    "- The weight is designed differently using softmax\n",
    "- Though here softmax would do the same thing as above by evenly distributing the weight aggregate\n",
    "- However, that is the case where we use 0 and 1, hence when we use weight that is data dependent (QKV in softmax), softmax would be better then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8121b3ec-a5ed-4ae9-8d03-b6163863cdfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T)) # because 8 tokens must result into 8X8\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6fe5a250-9c5f-4b1a-9568-7028afa73f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = torch.zeros(T,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2af0f658-54f9-4dd9-879d-77dcdafb422b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = scores.masked_fill(tril == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67619d42-6a0c-442f-96d7-d2e8ecab7c35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6b466c5-debd-4296-990b-1a4db0262365",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_attn = torch.softmax(scores, dim=-1)\n",
    "p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d8a6dac-48d3-4949-90db-21f0685a258e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention3 = p_attn @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4c2b658-c419-43a6-8c7d-bb39f37da430",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873],\n",
       "        [ 1.4138, -0.3091],\n",
       "        [ 1.1687, -0.6176],\n",
       "        [ 0.8657, -0.8644],\n",
       "        [ 0.5422, -0.3617],\n",
       "        [ 0.3864, -0.5354],\n",
       "        [ 0.2272, -0.5388],\n",
       "        [ 0.1027, -0.3762]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22f9b96b-5c6e-487e-afbf-4c59e7a16bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(attention2[0], attention3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30b0c2-dd05-4164-8a66-9867b4d232ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6816930-ce86-4a6d-8fcc-7a357ad152a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Version 3: Final with self attention (Putting everything together)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f4d72-339f-4768-9b37-346f42a53e97",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Here, the weight will be data dependent, thus making softmax very useful \n",
    "- The idea is, unlIke previous version, we do not want the values to be context average, otherwise it suggests that all token are equally important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65c4ca8-4805-47be-abc9-54f086ebe620",
   "metadata": {},
   "source": [
    "Note below: In terms of image, we are no longer talking about 2D image, thus, arrangement of Channel, Width, Height no longer matter, everything is now 1D. Hence,\n",
    "- B = batch, in image = channel\n",
    "- T = time/Sentence, in image = flattend R/G/B \n",
    "- C = Depth dimension, can be any value you wish e.g RGB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a5447ab-2628-45d1-8059-d7aa6d621056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2c84d-6107-4488-87f4-931532dec426",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Note: the QKV is for the weight initialization, hence must come out in block_size/context lenght size\n",
    "- The K and Q are the same values, but by using the transpose, each and every token can multiply its Key to all Query of the others\n",
    "- Then we can estimate its affinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34f953ee-8ec0-4403-b23c-d9406b8c04fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "head_size = 3 # 16 # this is the \n",
    "key = nn.Linear(in_features=C, out_features=head_size)\n",
    "query = nn.Linear(in_features=C, out_features=head_size)\n",
    "value = nn.Linear(in_features=C, out_features=head_size)\n",
    "k = key(x) # shape = (B, T, 3)\n",
    "q = query(x) # shape = (B, T, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4061e0d3-4237-4941-8f17-c9d220552f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a88a6de4-3ecb-439e-9f51-c0ca8260a4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 8])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(-2, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2410e3ef-5c12-40e4-9836-3600f299708a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matmul_qk = q@k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f6de4eb-46f4-4977-8233-979be4223233",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul_qk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df463f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3184, -0.7780, -0.0200,  0.0947, -0.1123, -0.5016, -0.9347,  0.2224],\n",
       "        [-0.5950, -1.0813,  0.2122, -0.0400, -0.5541, -0.0613, -1.3578,  0.3490],\n",
       "        [-0.2704, -0.9691,  0.0344,  0.3219,  0.0966, -0.7693, -1.1739,  0.4391],\n",
       "        [-0.1013, -0.5984, -0.1795,  0.2493,  0.2661, -0.8931, -0.6777,  0.1802],\n",
       "        [-0.1469, -0.5021, -0.1981,  0.1047,  0.1139, -0.7057, -0.5598,  0.0578],\n",
       "        [-0.6235, -1.4734,  0.3764,  0.2479, -0.4039, -0.2991, -1.8641,  0.7155],\n",
       "        [-0.6597, -1.0974,  0.2452, -0.1175, -0.6872,  0.0848, -1.3864,  0.3249],\n",
       "        [-0.0925, -0.7147, -0.1380,  0.3589,  0.3486, -1.0060, -0.8257,  0.2996]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul_qk[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fa4da112-2984-450a-804b-65178764843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = matmul_qk * k.size(-1)**-0.5 # Scale Factor : Square root of the key size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "29cc7045-6a41-4570-aa44-937f943771f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1838,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.3435, -0.6243,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.1561, -0.5595,  0.0199,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.0585, -0.3455, -0.1036,  0.1439,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.0848, -0.2899, -0.1144,  0.0605,  0.0658,    -inf,    -inf,    -inf],\n",
       "        [-0.3600, -0.8507,  0.2173,  0.1431, -0.2332, -0.1727,    -inf,    -inf],\n",
       "        [-0.3809, -0.6336,  0.1416, -0.0678, -0.3968,  0.0489, -0.8004,    -inf],\n",
       "        [-0.0534, -0.4126, -0.0797,  0.2072,  0.2012, -0.5808, -0.4767,  0.1729]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ad6b22-652e-48a8-8076-6d93ed31958a",
   "metadata": {},
   "source": [
    "- The above has made our initial weight to be data driven\n",
    "- Now we need to truncate using trill below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa2a367d-1eb1-4c81-bc84-c8f40c5142c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1838,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.3435, -0.6243,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.1561, -0.5595,  0.0199,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.0585, -0.3455, -0.1036,  0.1439,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.0848, -0.2899, -0.1144,  0.0605,  0.0658,    -inf,    -inf,    -inf],\n",
       "        [-0.3600, -0.8507,  0.2173,  0.1431, -0.2332, -0.1727,    -inf,    -inf],\n",
       "        [-0.3809, -0.6336,  0.1416, -0.0678, -0.3968,  0.0489, -0.8004,    -inf],\n",
       "        [-0.0534, -0.4126, -0.0797,  0.2072,  0.2012, -0.5808, -0.4767,  0.1729]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "scores = scores.masked_fill(tril == 0, float('-inf')) # Comment out to obtain bi-directional effect\n",
    "scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cf1c1de6-c0e0-424b-854b-081cd239e33d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5697, 0.4303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3496, 0.2335, 0.4169, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2544, 0.1909, 0.2432, 0.3115, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1959, 0.1596, 0.1902, 0.2265, 0.2278, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1353, 0.0829, 0.2411, 0.2238, 0.1536, 0.1632, 0.0000, 0.0000],\n",
       "        [0.1249, 0.0970, 0.2105, 0.1708, 0.1229, 0.1919, 0.0821, 0.0000],\n",
       "        [0.1289, 0.0900, 0.1255, 0.1673, 0.1663, 0.0761, 0.0844, 0.1616]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_attn = torch.softmax(scores, dim=-1)\n",
    "p_attn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7d8f662c-260b-4bc1-acef-26b87efbcf68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aa99856f-a585-4180-a078-2ff4bf8c7b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 3])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = value(x) # we aggregate the values not the exact token, it is also learnable\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "29f8dc61-ff33-42b9-b2ab-6e7579794263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention = p_attn @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ebf1dba8-8b3d-489d-bd19-71bab3c381e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 3])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f1c84-3f10-4478-83d6-54dfeeae76b2",
   "metadata": {},
   "source": [
    "## Puthing all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "562ca334-70e9-41ca-8b6e-b37fa2539843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        #self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # input of size (batch, latent-space, feature map)\n",
    "        # output of size (batch, latent-space, head size)\n",
    "        \n",
    "        B,T,C = key.shape\n",
    "        \n",
    "        key = self.key(key)   # (B,T,hs)\n",
    "        query = self.query(query) # (B,T,hs)\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        matmul_qk = query @ key.transpose(-2,-1) # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        \n",
    "        scores = matmul_qk * key.size(-1)**-0.5 # Scale Factor\n",
    "\n",
    "        scores = scores.masked_fill(torch.tril(torch.ones(T,T)) == 0, float('-inf'))# (B, T, T) # Comment out to obtain bi-directional effect\n",
    "        \n",
    "        p_attn = F.softmax(scores, dim=-1) # (B, T, T)\n",
    "        \n",
    "        p_attn = self.dropout(p_attn)\n",
    "        \n",
    "        # perform the weighted aggregation of the values\n",
    "        value = self.value(value) # (B,T,hs)\n",
    "        attention = p_attn @ value # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        \n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846e14d-2452-444c-b112-19165c2fc4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
